{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14cc33db-60c9-4165-93fe-7e8660870c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from WeiboCrawler import *\n",
    "from utils import *\n",
    "from Agents import Coordinator, SentimentAnalysistAgent, TopicModellingAgent, Summarizer\n",
    "import json\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f49204cf-1aff-4478-81cb-d1bbc314e89e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Coordinator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 122\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cleaned_summary_response\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# 第一步：对话获取参数，并启动微博爬虫\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     csv_file, event_keywords \u001b[38;5;241m=\u001b[39m \u001b[43mconversation_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(csv_file):\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;66;03m# 第二步：进行情感分析，累计情感统计\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         sentiment_counts \u001b[38;5;241m=\u001b[39m perform_sentiment_analysis(csv_file)\n",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m, in \u001b[0;36mconversation_loop\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconversation_loop\u001b[39m():\n\u001b[1;32m      8\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    与用户持续对话，直到 agent 返回包含所有必需信息的 JSON 格式回复，\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    此时调用微博爬虫，并返回生成的 CSV 文件名和事件关键字。\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     coordinator \u001b[38;5;241m=\u001b[39m \u001b[43mCoordinator\u001b[49m(prompt_filepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./prompts/Coordinator_prompt.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m         user_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m请输入查询内容：\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Coordinator' is not defined"
     ]
    }
   ],
   "source": [
    "def conversation_loop():\n",
    "    \"\"\"\n",
    "    与用户持续对话，直到 agent 返回包含所有必需信息的 JSON 格式回复，\n",
    "    此时调用微博爬虫，并返回生成的 CSV 文件名和事件关键字。\n",
    "    \"\"\"\n",
    "    coordinator = Coordinator(prompt_filepath='./prompts/Coordinator_prompt.txt')\n",
    "    \n",
    "    while True:\n",
    "        user_text = input(\"请输入查询内容：\")\n",
    "        # 调用 Coordinator 处理用户输入\n",
    "        response = coordinator.run(user_text)\n",
    "        response = clean_json_output(response)\n",
    "        print(\"Agent 回复：\", response)\n",
    "        \n",
    "        # 尝试解析回复为 JSON 格式\n",
    "        try:\n",
    "            result = json.loads(response)\n",
    "            required_keys = [\"event_keywords\", \"start_year\", \"start_month\", \"start_day\", \"event_release_platform\"]\n",
    "            if all(key in result for key in required_keys):\n",
    "                platform = result[\"event_release_platform\"].lower()\n",
    "                if platform not in [\"weibo\", \"微博\"]:\n",
    "                    print(\"目前服务仅支持微博平台，请重新提供相关信息。\")\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    year = int(result[\"start_year\"])\n",
    "                    month = int(result[\"start_month\"])\n",
    "                    day = int(result[\"start_day\"])\n",
    "                except Exception as e:\n",
    "                    print(\"时间信息格式有误，请检查后重试。\", e)\n",
    "                    continue\n",
    "\n",
    "                # 保存事件关键字（列表格式），取第一个关键字作为爬虫关键字\n",
    "                event_keywords = result[\"event_keywords\"]\n",
    "                keyword = event_keywords[0].strip(\"#\")\n",
    "                \n",
    "                # 回复格式完整时调用爬虫，并返回 CSV 文件名和事件关键字\n",
    "                csv_file = run_weibo_crawl(year, month, day, keyword)\n",
    "                print(\"微博爬虫已启动，爬取任务开始执行。\")\n",
    "                return csv_file, event_keywords\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "\n",
    "def perform_sentiment_analysis(csv_file):\n",
    "    \"\"\"\n",
    "    读取 CSV 文件，分批调用情感分析 agent，\n",
    "    并累计统计情感结果。返回 sentiment_counts 字典。\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_file, encoding='utf-8')\n",
    "    chunk_size = 10\n",
    "    num_rows = len(df)\n",
    "    sentiment_counts = {\"positive\": 0, \"neutral\": 0, \"negative\": 0}\n",
    "\n",
    "    for start in tqdm(range(0, num_rows, chunk_size), desc=\"Processing chunks\"):\n",
    "        agent = SentimentAnalysistAgent(\"./prompts/Sentiment_analysist_prompt.txt\")\n",
    "        \n",
    "        chunk = df.iloc[start:start + chunk_size].copy()\n",
    "        chunk.reset_index(drop=True, inplace=True)\n",
    "        # 为每条微博添加编号\n",
    "        chunk['编号'] = chunk.index + 1\n",
    "        \n",
    "        query_lines = chunk.apply(lambda row: f\"{row['编号']}: {row['微博正文']}\", axis=1)\n",
    "        query = \"\\n\".join(query_lines)\n",
    "        \n",
    "        response_str = agent.run(query)\n",
    "        cleaned_response = clean_json_output(response_str)\n",
    "        \n",
    "        try:\n",
    "            response_data = json.loads(cleaned_response)\n",
    "            summary = response_data.get(\"summary\", {})\n",
    "            sentiment_counts[\"positive\"] += summary.get(\"positive\", 0)\n",
    "            sentiment_counts[\"neutral\"] += summary.get(\"neutral\", 0)\n",
    "            sentiment_counts[\"negative\"] += summary.get(\"negative\", 0)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(\"JSON解析失败:\", e)\n",
    "\n",
    "    print(\"累计情感统计:\")\n",
    "    print(\"Positive:\", sentiment_counts[\"positive\"])\n",
    "    print(\"Neutral:\", sentiment_counts[\"neutral\"])\n",
    "    print(\"Negative:\", sentiment_counts[\"negative\"])\n",
    "    return sentiment_counts\n",
    "\n",
    "def summarize_event(event_keywords, sentiment_counts):\n",
    "    \"\"\"\n",
    "    根据事件关键字和累积情感统计生成总结的 prompt，\n",
    "    并调用 Summarizer agent 返回清洗后的事件总结。\n",
    "    \n",
    "    参数:\n",
    "        event_keywords: 事件关键字列表（例如 [\"#金价上涨#\"]，取第一个关键字）\n",
    "        sentiment_counts: 累积情感统计字典，包含 \"positive\", \"neutral\", \"negative\"\n",
    "        \n",
    "    返回:\n",
    "        Summarizer 生成的事件总结文本（清洗后的结果）\n",
    "    \"\"\"\n",
    "    if isinstance(event_keywords, list):\n",
    "        event_keyword = event_keywords[0]\n",
    "    else:\n",
    "        event_keyword = event_keywords\n",
    "\n",
    "    prompt = (\n",
    "        f\"请帮我对 {event_keyword} 事件进行总结，它的累计情感统计:\\n\"\n",
    "        f\"Positive: {sentiment_counts['positive']}\\n\"\n",
    "        f\"Neutral: {sentiment_counts['neutral']}\\n\"\n",
    "        f\"Negative: {sentiment_counts['negative']}\"\n",
    "    )\n",
    "    print(prompt)\n",
    "    summarizer = Summarizer(\"./prompts/Summarizer.txt\")\n",
    "    summarizer_response = summarizer.run(prompt)\n",
    "    cleaned_summary_response = clean_json_output(summarizer_response)\n",
    "    \n",
    "    return cleaned_summary_response\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 第一步：对话获取参数，并启动微博爬虫\n",
    "    csv_file, event_keywords = conversation_loop()\n",
    "    \n",
    "    if os.path.exists(csv_file):\n",
    "        # 第二步：进行情感分析，累计情感统计\n",
    "        sentiment_counts = perform_sentiment_analysis(csv_file)\n",
    "        # 第三步：生成事件总结并调用 Summarizer 生成最终总结\n",
    "        summary = summarize_event(event_keywords, sentiment_counts)\n",
    "        print(\"总结回复：\", summary)\n",
    "    else:\n",
    "        print(\"爬虫生成的 CSV 文件不存在，请检查。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93430cbc-7a99-4479-9cb9-4932406c5cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8b0552-8dd9-480b-b6aa-4bb43ed9ec28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5fc9eb-ee14-4e40-aa48-17f6e7159a00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6e1a5dd-2b52-41a1-a8a1-61ceba5b3b5a",
   "metadata": {},
   "source": [
    "#金价# 事件的情感统计分析表明，整体舆论趋向于正面评价。在所有相关帖子中，**正面情绪占据主导地位，共有 17 条正面帖文，占比 40.5%**。这表明多数公众对黄金价格相关话题持有积极态度，可能是由于近期金价上涨，带来了投资者的乐观情绪，或者黄金市场的稳定性增强了公众信心。  \n",
    "\n",
    "**中立情绪略占优势，共 23 条，占比 54.8%**。这表明相当一部分网民更多是在客观讨论金价走势，而非直接呈现强烈的情感反应。可能的原因包括人们关注市场动态，但持谨慎观察态度，尚未表现出明确的情绪倾向。  \n",
    "\n",
    "**负面情绪最少，仅 2 条，占比 4.8%**，显示出公众对于金价话题的负面反应很少。这或许意味着当前的金价变动并未引起市场的广泛担忧，或者金价的走势对大部分人而言尚未产生明显的不利影响。  \n",
    "\n",
    "综合来看，本次 #金价# 的舆论环境较为健康，以中立和正面情绪为主，负面讨论极少。若金价继续呈稳定甚至上涨趋势，市场信心或将进一步增强，带动更多正面情绪的涌现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ecfeb00-2acd-48e1-933d-5027b7b7f6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_topic_analysis(csv_file: str, initial_topics: list):\n",
    "    \"\"\"\n",
    "    读取 CSV 文件，分批调用 TopicModellingAgent 进行主题分析，\n",
    "    并根据返回结果更新主题列表。返回更新后的主题列表。\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_file, encoding='utf-8')\n",
    "    chunk_size = 10\n",
    "    num_rows = len(df)\n",
    "    topics = initial_topics.copy()  # 初始化主题列表\n",
    "\n",
    "    for start in tqdm(range(0, num_rows, chunk_size), desc=\"Processing chunks\"):\n",
    "        # 每个批次重新实例化 agent，避免对话历史过长\n",
    "        agent = TopicModellingAgent(\"./prompts/Topic_modelling_prompt.txt\")\n",
    "        \n",
    "        # 取出当前块数据，并为每条数据添加编号\n",
    "        chunk = df.iloc[start:start + chunk_size].copy()\n",
    "        chunk.reset_index(drop=True, inplace=True)\n",
    "        chunk['编号'] = chunk.index + 1\n",
    "        \n",
    "        # 拼接每条公民意见数据（假设 CSV 中的“微博正文”列包含公民意见）\n",
    "        query_lines = chunk.apply(lambda row: f\"{row['编号']}: {row['微博正文']}\", axis=1)\n",
    "        opinions_text = \"\\n\".join(query_lines)\n",
    "        \n",
    "        # 构造 query：包含当前批次的公民意见和已确定的主题列表\n",
    "        topics_str = \", \".join(topics) if topics else \"无\"\n",
    "        query = (\n",
    "            f\"请基于以下公民意见数据和当前主题列表进行主题分析：\\n\\n\"\n",
    "            f\"公民意见：\\n{opinions_text}\\n\\n\"\n",
    "            f\"当前主题列表：{topics_str}\\n\\n\"\n",
    "        )\n",
    "        \n",
    "        response_str = agent.run(query)\n",
    "        cleaned_response = clean_json_output(response_str)\n",
    "        print(cleaned_response)\n",
    "        \n",
    "        try:\n",
    "            response_data = json.loads(cleaned_response)\n",
    "            analyses = response_data.get(\"analyses\", [])\n",
    "            for item in analyses:\n",
    "                topic = item.get(\"topic\", \"\").strip()\n",
    "                # 如果 topic 非空且不在已有列表中，则添加到列表\n",
    "                if topic and topic not in topics:\n",
    "                    topics.append(topic)\n",
    "                    print(topics)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(\"JSON解析失败:\", e)\n",
    "\n",
    "    print(\"更新后的主题列表:\")\n",
    "    print(topics)\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6090a1cb-ee1b-4b6e-a338-4496ebde054b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████████████████████| 5/5 [00:56<00:00, 11.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "更新后的主题列表:\n",
      "['深圳疫情']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "updated_topics = perform_topic_analysis(\"50sample.csv\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff6bc2b-7636-4d8c-8846-173945155433",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
